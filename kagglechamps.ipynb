{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def load_dataset(path_to_file):\n",
    "    X, y = [], []\n",
    "    with codecs.open(path_to_file, \"r\",encoding='utf-8', errors='ignore') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        next(reader, None) # Skip header\n",
    "        for row in reader:\n",
    "            y.append(int(row[1]))\n",
    "            X.append(row[2])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64774    @BradtheGleek It's been dead to you? Not here....\n",
      "43321    @AlexReside  Ooops.. I had a bug   But I fixed...\n",
      "44030                              @Anistorm Whats wrong? \n",
      "45942            @alittletrendy sad  where's your laptop?!\n",
      "71782    @cakey oh that's good, not bad thanks altho no...\n",
      "62719    @bikerchick22 what's up hottie!  great to see ...\n",
      "84527    @Cattington No, not yet,.... lots of wind and ...\n",
      "51064                @asyeasyeasye haloo too  how are you?\n",
      "67435    @brookandthecity Lol, oh ok. Just making sure ...\n",
      "99784    @Crystal_ESPN Ah! OK! Wanted to make sure! Hil...\n",
      "22086    @_Larissa_ HAHa normally yeah BUT when its PP ...\n",
      "12277    -&gt; @glynmoody my poor Spanish leaves me com...\n",
      "70442    @breezyballababe oh awwwww dats how ima feel i...\n",
      "69633                         @andisherwood I like that.  \n",
      "83114    @AshPash Awesome job on getting up and doing y...\n",
      "81036                           @asdfology What about me. \n",
      "78646    @cecedesouza If u need help in the 7osa oo loy...\n",
      "70830      @andybudd Looks like you beat me to the punch. \n",
      "33942    @Alli_Flowers I stopped spelling it Ren√© onlin...\n",
      "640                            water works are gonna start\n",
      "77305    @catfish_ohm i highly doubt it's dead...its po...\n",
      "57635        @BecM49 i am, thanks (: i'll put you in mine \n",
      "81348    @carlosfoxtv What's sad about Gokey is the sto...\n",
      "53176       @AztecBeast We are driving there now.  foreal!\n",
      "76359                         @cascandar You can annoy me \n",
      "79175    @ArmandoRJ  They're funny little Banditos aren...\n",
      "36277    @AlexaRPD My brother and I miss you over at Fa...\n",
      "56243           @BaleBabe66 Red Robin closed where I live \n",
      "26373    @a_dubstar after you buy me a new foot pedal ....\n",
      "69973    @BrandyandIce there not playing till 5.15, can...\n",
      "62388          @BLaCkitaLiaNa7 ay uuuu  happy mothers day \n",
      "22724    @1000wattmarc well that's the problem with thi...\n",
      "86241         @chrisquinn2110 got to love those got tubs! \n",
      "19085             :O we're not eating sushi for dinnnaarr \n",
      "80413    @carcarly oh..... i know.. they do in Singapor...\n",
      "40883               @Amy_BabyGirl  how's the new sidekick?\n",
      "48356    @Artfullife4 thank you so much for the shout o...\n",
      "30905    @Adrigonzo I know right? There is a point to p...\n",
      "63011    @blogomomma I've been spending extra time with...\n",
      "27498    @agirlwhocan what happened! But summary to the...\n",
      "65365    @Bren_311 True! We all have different answers!...\n",
      "36025                  @ale_grillita Good luck with it!!  \n",
      "30576    @adreamreaves I gave in and turned my heat on ...\n",
      "37786    @amandaux Thanks, Amanda. I'll check it out la...\n",
      "41547    @ananyah i havent worked out how to do pics ye...\n",
      "98193           @conorgodfrey Yes Conold I am distressed! \n",
      "57294    @BayAreaCheap there r a few of us local yocals...\n",
      "83378    @CassieFX well but the other doctor said it wa...\n",
      "31243    @AlexAllTimeLow sadly no  lets still get waste...\n",
      "15333                                        *MUM called* \n",
      "Name: data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x, y = load_dataset('data/train.csv')\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "indices = np.random.choice(range(100000), 10000, replace=False)\n",
    "\n",
    "y_series = pd.Series(y, name='labels')\n",
    "X_series = pd.Series(x, name='data')\n",
    "\n",
    "X_train = X_series[indices]\n",
    "y_train = y_series[indices]\n",
    "print(X_train[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 5 rows of data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64774    @BradtheGleek It's been dead to you? Not here....\n",
      "43321    @AlexReside  Ooops.. I had a bug   But I fixed...\n",
      "44030                              @Anistorm Whats wrong? \n",
      "45942            @alittletrendy sad  where's your laptop?!\n",
      "71782    @cakey oh that's good, not bad thanks altho no...\n",
      "Name: data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First solution\n",
    "Out first approach is to hash every tweet and check if there's some correlation between !@#!#!@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "z = [int(hashlib.sha1(string.encode('utf-8')).hexdigest(), 16) for string in x]\n",
    "z = np.array(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021594023517841287183518356032487246803893750680\n"
     ]
    }
   ],
   "source": [
    "print(z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# spliting the data into train/test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(z, y, test_size=0.2, random_state=23)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test.reshape(-1, 1))\n",
    "y_diff = y_pred == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5622062206220622\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for qwe in y_diff:\n",
    "    if qwe:\n",
    "        cnt += 1\n",
    "print(cnt/len(y_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second and serious solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_red = pd.Series(X_train)\n",
    "y_red = pd.Series(y_train)\n",
    "# X_red = pd.Series(x[10000:20000])\n",
    "# y_red = pd.Series(y[10000:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "[64774 43321 44030 ..., 62022 71362 33217]\n"
     ]
    }
   ],
   "source": [
    "print(type(X_red[indices[0]]))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "- Data cleaning\n",
    "    - Delete links\n",
    "    - Delete mentions\n",
    "    - Remove punctation\n",
    "    - All lowercase\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mi'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data.spellcheck\n",
    "data.spellcheck.correction('radras')\n",
    "data.spellcheck.correction('mi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha omg. stayin steezy & mowin the lawn... loviie still here && goiing tanning soon.\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "print(html.unescape('haha omg. stayin steezy &amp; mowin the lawn... loviie still here &amp;&amp; goiing tanning soon.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It's been dead to you? Not here. My friend tried making an account, and when he did, 2 minutes later his page didn't exist \n",
      "  Ooops.. I had a bug   But I fixed it..    Try that tiny URL again..\n",
      " Whats wrong? \n",
      " sad  where's your laptop?!\n",
      " oh that's good, not bad thanks altho not wanting to go to work \n",
      " what's up hottie!  great to see you tweetin in!!! \n",
      " No, not yet,.... lots of wind and rain and cold  How are you my friend?\n",
      " haloo too  how are you?\n",
      " Lol, oh ok. Just making sure i was still in tuned wit dat good ol' ol skool muzik! \n",
      " Ah! OK! Wanted to make sure! Hilarious. Yeah, she's the homie alright. But she better watch out. I know way too much, \n"
     ]
    }
   ],
   "source": [
    "# Removing twitter mentions\n",
    "for i in indices:\n",
    "    X_red[i] = re.sub(r'@\\w+', \" \", str(X_red[i]))\n",
    "    \n",
    "# Converting HTML codes to plain text\n",
    "for i in indices:\n",
    "    X_red[i] = html.unescape(X_red[i])\n",
    "for i in range(10):\n",
    "    print(X_red[indices[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('correct.v.01'),\n",
       " Synset('right.v.01'),\n",
       " Synset('chastise.v.01'),\n",
       " Synset('compensate.v.01'),\n",
       " Synset('discipline.v.02'),\n",
       " Synset('decline.v.06'),\n",
       " Synset('adjust.v.01'),\n",
       " Synset('correct.v.08'),\n",
       " Synset('correct.a.01'),\n",
       " Synset('correct.s.02'),\n",
       " Synset('correct.s.03'),\n",
       " Synset('right.a.05')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(data.spellcheck.correction('korrekt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "['dead', 'friend', 'tried', 'making', 'account', 'minutes', 'later', 'page', 'exist']\n",
      "['oops', 'bug', 'fixed', 'try', 'tiny', 'url']\n",
      "['whats', 'wrong']\n",
      "['sad', \"where's\", 'laptop']\n",
      "['oh', \"that's\", 'good', 'bad', 'thanks', 'altho', 'wanting', 'go', 'work']\n",
      "[\"what's\", 'hottie', 'great', 'see', 'tweetin']\n",
      "['yet', 'lots', 'wind', 'rain', 'cold', 'friend']\n",
      "['haloo']\n",
      "['lol', 'oh', 'ok', 'making', 'sure', 'still', 'tuned', 'wit', 'dat', 'good', \"ol'\", 'ol', 'skool', 'muzik']\n",
      "['ah', 'ok', 'wanted', 'make', 'sure', 'hilarious', 'yeah', 'homie', 'alright', 'better', 'watch', 'know', 'way', 'much']\n",
      "\n",
      "LEN: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from data.utils import *\n",
    "import autocorrect\n",
    "\n",
    "# forms list of sentences per every tweet\n",
    "all_words = [regexp_tokenize(red, \"[\\w']+\") for red in X_red]\n",
    "\n",
    "for i in range(len(all_words)):\n",
    "    for j in range(len(all_words[i])):\n",
    "        all_words[i][j] = remove_more_than_two_duplicate_letters(all_words[i][j].lower())\n",
    "print(\"finished\")\n",
    "\n",
    "# clean the set from the numeric values\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [word for word in all_words[i] if not word.isnumeric()]\n",
    "    all_words[i] = [word for word in all_words[i] if word not in stopwords.words('english')]\n",
    "\n",
    "for i in range(10):\n",
    "    print(all_words[i])\n",
    "    \n",
    "print()\n",
    "print('LEN:', len(stopwords.words('english')))\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "\n",
    "#.. Omgaga. Im sooo  im gunna CRy. \n",
    "#I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
    "\n",
    "\n",
    "# for i in range(len(all_sentences)):\n",
    "#     for j in range(all_sentences[i]):\n",
    "#         all_sentences[i][j] = regexp_tokenize(sentence, \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dead', 'friend', 'tri', 'make', 'account', 'minut', 'later', 'page', 'exist']\n",
      "['oop', 'bug', 'fix', 'tri', 'tini', 'url']\n",
      "['what', 'wrong']\n",
      "['sad', \"where'\", 'laptop']\n",
      "['oh', \"that'\", 'good', 'bad', 'thank', 'altho', 'want', 'go', 'work']\n",
      "[\"what'\", 'hotti', 'great', 'see', 'tweetin']\n",
      "['yet', 'lot', 'wind', 'rain', 'cold', 'friend']\n",
      "['haloo']\n",
      "['lol', 'oh', 'ok', 'make', 'sure', 'still', 'tune', 'wit', 'dat', 'good', \"ol'\", 'ol', 'skool', 'muzik']\n",
      "['ah', 'ok', 'want', 'make', 'sure', 'hilari', 'yeah', 'homi', 'alright', 'better', 'watch', 'know', 'way', 'much']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "for i in range(len(all_words)):\n",
    "    for j in range(len(all_words[i])):\n",
    "        all_words[i][j] = porter.stem(all_words[i][j])\n",
    "        \n",
    "for i in range(10):\n",
    "    print(all_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dead friend tri make account minut later page exist', 'oop bug fix tri tini url', 'what wrong', \"sad where' laptop\", \"oh that' good bad thank altho want go work\"]\n"
     ]
    }
   ],
   "source": [
    "allz_words = []\n",
    "for lista in all_words:\n",
    "    allz_words.append(\" \".join(lista))\n",
    "\n",
    "print(allz_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(allz_words)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "y_nov = []\n",
    "for i in indices:\n",
    "    y_nov.append(y_red[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d96a2087fc83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, y_nov, test_size=0.2, random_state=23)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_diff = y_pred == y_test\n",
    "\n",
    "print(len(train_data_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for qwe in y_diff:\n",
    "    if qwe:\n",
    "        cnt += 1\n",
    "print(cnt/len(y_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=101)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_diff = y_pred == y_test\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for qwe in y_diff:\n",
    "    if qwe:\n",
    "        cnt += 1\n",
    "print(cnt/len(y_diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21766   NaN\n",
      "31167   NaN\n",
      "97526   NaN\n",
      "37538   NaN\n",
      "58234   NaN\n",
      "22041   NaN\n",
      "22256   NaN\n",
      "29630   NaN\n",
      "72726   NaN\n",
      "53980   NaN\n",
      "55329   NaN\n",
      "39064   NaN\n",
      "29649   NaN\n",
      "25675   NaN\n",
      "20126   NaN\n",
      "59010   NaN\n",
      "53061   NaN\n",
      "14594   NaN\n",
      "53302   NaN\n",
      "83047   NaN\n",
      "13809   NaN\n",
      "62194   NaN\n",
      "73472   NaN\n",
      "32456   NaN\n",
      "16359   NaN\n",
      "74733   NaN\n",
      "11300   NaN\n",
      "12952   NaN\n",
      "21527   NaN\n",
      "22394   NaN\n",
      "56750   NaN\n",
      "40941   NaN\n",
      "76221   NaN\n",
      "88124   NaN\n",
      "8948    NaN\n",
      "22664   NaN\n",
      "69776   NaN\n",
      "71499   NaN\n",
      "19327   NaN\n",
      "65972   NaN\n",
      "10189   NaN\n",
      "48833   NaN\n",
      "78504   NaN\n",
      "8345    NaN\n",
      "59089   NaN\n",
      "52461   NaN\n",
      "29548   NaN\n",
      "32516   NaN\n",
      "72157   NaN\n",
      "26261   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "indices = np.random.choice(range(100000), 1000, replace=False)\n",
    "\n",
    "\n",
    "y_series = pd.Series(y)\n",
    "X_series = pd.Series(x)\n",
    "\n",
    "X_train = X_series[indices]\n",
    "print(X_train[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
