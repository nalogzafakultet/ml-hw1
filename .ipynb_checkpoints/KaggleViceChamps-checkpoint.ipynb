{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter sentiment presented by @Radras and @sekularacn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import html\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_file):\n",
    "    X, y = [], []\n",
    "    with codecs.open(path_to_file, \"r\",encoding='utf-8', errors='ignore') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        next(reader, None) # Skip header\n",
    "        for row in reader:\n",
    "            y.append(int(row[1]))\n",
    "            X.append(row[2])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of data: 99989  Len of labels: 99989\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = load_dataset('data/train.csv')\n",
    "\n",
    "print('Len of data:', len(x_data), ' Len of labels:', len(y_data))\n",
    "\n",
    "# Getting the random data from the dataset.\n",
    "# \n",
    "indices = np.random.choice(range(99989), 10000, replace=False)\n",
    "\n",
    "X_random_data = [x_data[i] for i in indices]\n",
    "y_random_labels = [y_data[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "- Deleting mentions\n",
    "- Deleting links\n",
    "- Fixing the HTML symbols for unicode chars\n",
    "- Removing punctation\n",
    "- Lowercasing all\n",
    "- Removing all the numeric values\n",
    "- Removing stopwords\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting links. Tokenizers don't work.\n",
    "for i in range(len(X_random_data)):\n",
    "    sentence = X_random_data[i].split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word[:4] != 'http':\n",
    "            new_sentence.append(word)\n",
    "    X_random_data[i] = ' '.join(new_sentence)\n",
    "\n",
    "for i in range(len(X_random_data)):\n",
    "    # Removing twitter mentions\n",
    "    X_random_data[i] = re.sub(r'@\\w+', \"\", X_random_data[i])\n",
    "    \n",
    "    # Removing HTML escaped symbols\n",
    "    X_random_data[i] = html.unescape(X_random_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "[['wait', 'your', 'hubby', 'is', 'at', 'the', 'show', 'and', 'your', 'not', 'why'], ['at', 'the', '1', 'u', 'just', 'mentioned', 'i', 'was', 'just', 'wait', 'till', 'the', 'wedding', 'speech', \"i'll\", 'b', 'uncontrollable'], [\"can't\", 'you', 'make', 'it'], ['and', 'what', 'about', 'you', 'lovely', 'lady', 'busy', 'still', 'up', 'so', 'much', 'on', 'your', 'plate', \"when's\", 'our', 'date'], ['looks', 'like', 'the', 'links', 'are', 'broken']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from data.utils import *\n",
    "\n",
    "# forms list of sentences per every tweet\n",
    "all_words = [regexp_tokenize(sample, \"[\\w']+\") for sample in X_random_data]\n",
    "\n",
    "# removes words with more than 2 same chars, and converts to lowercase\n",
    "for i in range(len(all_words)):\n",
    "    for j in range(len(all_words[i])):\n",
    "        all_words[i][j] = remove_more_than_two_duplicate_letters(all_words[i][j].lower())\n",
    "print(\"finished\")\n",
    "\n",
    "print(all_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning numbers and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the set from the numeric values and remove stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [word for word in all_words[i] if not word.isnumeric()]\n",
    "    all_words[i] = [word for word in all_words[i] if word not in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'hubby', 'show']\n",
      "['u', 'mentioned', 'wait', 'till', 'wedding', 'speech', \"i'll\", 'b', 'uncontrollable']\n",
      "[\"can't\", 'make']\n",
      "['lovely', 'lady', 'busy', 'still', 'much', 'plate', \"when's\", 'date']\n",
      "['looks', 'like', 'links', 'broken']\n",
      "['lj', 'sweetjamielee', 'livejournal', 'com', 'sth', 'like']\n",
      "[\"that's\", 'one', 'cool', 'collection']\n",
      "['know', \"i'm\", 'excited', 'think', 'make', 'big', 'ejami', 'sign', 'find', 'lol']\n",
      "['sorry', 'loss']\n",
      "['freezing', 'buttocks']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(all_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Using PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'hubbi', 'show']\n",
      "['u', 'mention', 'wait', 'till', 'wed', 'speech', \"i'll\", 'b', 'uncontrol']\n",
      "[\"can't\", 'make']\n",
      "['love', 'ladi', 'busi', 'still', 'much', 'plate', \"when'\", 'date']\n",
      "['look', 'like', 'link', 'broken']\n",
      "['lj', 'sweetjamiele', 'livejourn', 'com', 'sth', 'like']\n",
      "[\"that'\", 'one', 'cool', 'collect']\n",
      "['know', \"i'm\", 'excit', 'think', 'make', 'big', 'ejami', 'sign', 'find', 'lol']\n",
      "['sorri', 'loss']\n",
      "['freez', 'buttock']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "for i in range(len(all_words)):\n",
    "    for j in range(len(all_words[i])):\n",
    "        all_words[i][j] = porter.stem(all_words[i][j])\n",
    "        \n",
    "for i in range(10):\n",
    "    print(all_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait hubbi show', \"u mention wait till wed speech i'll b uncontrol\", \"can't make\", \"love ladi busi still much plate when' date\", 'look like link broken']\n"
     ]
    }
   ],
   "source": [
    "# Repositioning the sentences\n",
    "\n",
    "clean_sentences = []\n",
    "for lista in all_words:\n",
    "    clean_sentences.append(\" \".join(lista))\n",
    "\n",
    "print(clean_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_sentences)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, y_random_labels, test_size=0.2, random_state=23)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_diff = y_pred == y_test\n",
    "\n",
    "cnt = 0\n",
    "for b in y_diff:\n",
    "    if b:\n",
    "        cnt += 1\n",
    "print((cnt / len(y_diff)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=101)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_diff = y_pred == y_test\n",
    "\n",
    "cnt = 0\n",
    "for qwe in y_diff:\n",
    "    if qwe:\n",
    "        cnt += 1\n",
    "print(cnt/len(y_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing dataset into train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 5000\n",
      "X_validate shape: 2500\n",
      "X_test shape: 2500\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, np.array(y_random_labels), test_size=0.5, random_state=23)\n",
    "\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, test_size=0.5, random_state=23)\n",
    "\n",
    "print('X_train shape:', len(X_train))\n",
    "print('X_validate shape:', len(X_validate))\n",
    "print('X_test shape:', len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "LEARNING_RATE = 0.5\n",
    "TRAINING_EPOCHS = 5000\n",
    "REGULARIZATION_PARAM = tf.constant(0.01)\n",
    "# CRITICAL::: X_TRAIN[0] MUST EXIST FOR THIS TO WORK PROPERLY\n",
    "N_FEATURES = len(X_train[0])\n",
    "\n",
    "y_train = y_train.reshape(len(y_train), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, N_FEATURES], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "\n",
    "Weights = tf.Variable(tf.random_normal([N_FEATURES, 1]), name=\"Weights\")\n",
    "bias = tf.Variable(tf.random_normal([1, 1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "hypotesis = tf.sigmoid(bias + tf.matmul(X, Weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function & L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "cost = tf.reduce_mean(-tf.multiply(y, tf.log(hypotesis)) - \n",
    "                      tf.multiply(tf.subtract(1.0, y), tf.log(tf.subtract(1.0, hypotesis)))\n",
    "                      + tf.multiply(REGULARIZATION_PARAM, Weights * Weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.32106\n",
      "100 0.999278\n",
      "200 0.926831\n",
      "300 0.873038\n",
      "400 0.830877\n",
      "500 0.796437\n",
      "600 0.767451\n",
      "700 0.742514\n",
      "800 0.720699\n",
      "900 0.701355\n",
      "1000 0.68401\n",
      "1100 0.668315\n",
      "1200 0.654005\n",
      "1300 0.64087\n",
      "1400 0.628747\n",
      "1500 0.617502\n",
      "1600 0.607025\n",
      "1700 0.597228\n",
      "1800 0.588034\n",
      "1900 0.579379\n",
      "2000 0.571207\n",
      "2100 0.563473\n",
      "2200 0.556136\n",
      "2300 0.54916\n",
      "2400 0.542515\n",
      "2500 0.536172\n",
      "2600 0.530109\n",
      "2700 0.524303\n",
      "2800 0.518737\n",
      "2900 0.513393\n",
      "3000 0.508255\n",
      "3100 0.503312\n",
      "3200 0.498549\n",
      "3300 0.493957\n",
      "3400 0.489524\n",
      "3500 0.485243\n",
      "3600 0.481104\n",
      "3700 0.477101\n",
      "3800 0.473225\n",
      "3900 0.469471\n",
      "4000 0.465832\n",
      "4100 0.462303\n",
      "4200 0.45888\n",
      "4300 0.455556\n",
      "4400 0.452326\n",
      "4500 0.449188\n",
      "4600 0.446137\n",
      "4700 0.443169\n",
      "4800 0.44028\n",
      "4900 0.437467\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        _, err = sess.run([train_op, cost], {X: X_train, y: y_train})\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, err)\n",
    "    W_computed, b_computed = sess.run([Weights, bias])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and testing for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.56 %\n"
     ]
    }
   ],
   "source": [
    "#hypotesis = tf.sigmoid(bias + tf.matmul(X, Weights))\n",
    "h = tf.sigmoid(b_computed + tf.matmul(X, W_computed))\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_predict = sess.run(h, {X: X_test})\n",
    "    y_predict_predict = []\n",
    "    \n",
    "    for value in y_predict:\n",
    "        if np.mean(value) >= 0.5:\n",
    "            y_predict_predict.append(1)\n",
    "        else:\n",
    "            y_predict_predict.append(0)\n",
    "    \n",
    "    y_diff = y_predict_predict == y_test\n",
    "    \n",
    "    for value in y_diff:\n",
    "        if value:\n",
    "            cnt += 1\n",
    "    \n",
    "    print('Accuracy:', cnt/(len(y_diff)) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
