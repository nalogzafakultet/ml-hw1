{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter sentiment presented by @Radras and @sekularacn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import html\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_file):\n",
    "    X, y = [], []\n",
    "    with codecs.open(path_to_file, \"r\",encoding='utf-8', errors='ignore') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        next(reader, None) # Skip header\n",
    "        for row in reader:\n",
    "            y.append(int(row[1]))\n",
    "            X.append(row[2])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of data: 99989  Len of labels: 99989\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = load_dataset('data/train.csv')\n",
    "\n",
    "print('Len of data:', len(x_data), ' Len of labels:', len(y_data))\n",
    "\n",
    "# Getting the random data from the dataset.\n",
    "# \n",
    "indices = np.random.choice(range(99989), 10000, replace=False)\n",
    "\n",
    "X_random_data = [x_data[i] for i in indices]\n",
    "y_random_labels = [y_data[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "- Deleting mentions\n",
    "- Deleting links\n",
    "- Fixing the HTML symbols for unicode chars\n",
    "- Removing punctation\n",
    "- Lowercasing all\n",
    "- Removing all the numeric values\n",
    "- Removing stopwords\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting links. Tokenizers don't work.\n",
    "for i in range(len(X_random_data)):\n",
    "    sentence = X_random_data[i].split(' ')\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        if word[:4] != 'http':\n",
    "            new_sentence.append(word)\n",
    "    X_random_data[i] = ' '.join(new_sentence)\n",
    "\n",
    "for i in range(len(X_random_data)):\n",
    "    # Removing twitter mentions\n",
    "    X_random_data[i] = re.sub(r'@\\w+', \"\", X_random_data[i])\n",
    "    \n",
    "    # Removing HTML escaped symbols\n",
    "    X_random_data[i] = html.unescape(X_random_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "[['wait', 'your', 'hubby', 'is', 'at', 'the', 'show', 'and', 'your', 'not', 'why'], ['at', 'the', '1', 'u', 'just', 'mentioned', 'i', 'was', 'just', 'wait', 'till', 'the', 'wedding', 'speech', \"i'll\", 'b', 'uncontrollable'], [\"can't\", 'you', 'make', 'it'], ['and', 'what', 'about', 'you', 'lovely', 'lady', 'busy', 'still', 'up', 'so', 'much', 'on', 'your', 'plate', \"when's\", 'our', 'date'], ['looks', 'like', 'the', 'links', 'are', 'broken']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from data.utils import *\n",
    "\n",
    "# forms list of sentences per every tweet\n",
    "all_words = [regexp_tokenize(sample, \"[\\w']+\") for sample in X_random_data]\n",
    "\n",
    "# removes words with more than 2 same chars, and converts to lowercase\n",
    "for i in range(len(all_words)):\n",
    "    for j in range(len(all_words[i])):\n",
    "        all_words[i][j] = remove_more_than_two_duplicate_letters(all_words[i][j].lower())\n",
    "print(\"finished\")\n",
    "\n",
    "print(all_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning numbers and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the set from the numeric values and remove stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [word for word in all_words[i] if not word.isnumeric()]\n",
    "    all_words[i] = [word for word in all_words[i] if word not in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'hubby', 'show']\n",
      "['u', 'mentioned', 'wait', 'till', 'wedding', 'speech', \"i'll\", 'b', 'uncontrollable']\n",
      "[\"can't\", 'make']\n",
      "['lovely', 'lady', 'busy', 'still', 'much', 'plate', \"when's\", 'date']\n",
      "['looks', 'like', 'links', 'broken']\n",
      "['lj', 'sweetjamielee', 'livejournal', 'com', 'sth', 'like']\n",
      "[\"that's\", 'one', 'cool', 'collection']\n",
      "['know', \"i'm\", 'excited', 'think', 'make', 'big', 'ejami', 'sign', 'find', 'lol']\n",
      "['sorry', 'loss']\n",
      "['freezing', 'buttocks']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(all_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Using PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'hubbi', 'show']\n",
      "['u', 'mention', 'wait', 'till', 'wed', 'speech', \"i'll\", 'b', 'uncontrol']\n",
      "[\"can't\", 'make']\n",
      "['love', 'ladi', 'busi', 'still', 'much', 'plate', \"when'\", 'date']\n",
      "['look', 'like', 'link', 'broken']\n",
      "['lj', 'sweetjamiele', 'livejourn', 'com', 'sth', 'like']\n",
      "[\"that'\", 'one', 'cool', 'collect']\n",
      "['know', \"i'm\", 'excit', 'think', 'make', 'big', 'ejami', 'sign', 'find', 'lol']\n",
      "['sorri', 'loss']\n",
      "['freez', 'buttock']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "for i in range(len(all_words)):\n",
    "    for j in range(len(all_words[i])):\n",
    "        all_words[i][j] = porter.stem(all_words[i][j])\n",
    "        \n",
    "for i in range(10):\n",
    "    print(all_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait hubbi show', \"u mention wait till wed speech i'll b uncontrol\", \"can't make\", \"love ladi busi still much plate when' date\", 'look like link broken']\n"
     ]
    }
   ],
   "source": [
    "# Repositioning the sentences\n",
    "\n",
    "clean_sentences = []\n",
    "for lista in all_words:\n",
    "    clean_sentences.append(\" \".join(lista))\n",
    "\n",
    "print(clean_sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_sentences)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, y_random_labels, test_size=0.2, random_state=23)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_diff = y_pred == y_test\n",
    "\n",
    "cnt = 0\n",
    "for b in y_diff:\n",
    "    if b:\n",
    "        cnt += 1\n",
    "print((cnt / len(y_diff)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=101)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_diff = y_pred == y_test\n",
    "\n",
    "cnt = 0\n",
    "for qwe in y_diff:\n",
    "    if qwe:\n",
    "        cnt += 1\n",
    "print(cnt/len(y_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing dataset into train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: 5000\n",
      "X_validate shape: 2500\n",
      "X_test shape: 2500\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, np.array(y_random_labels), test_size=0.5, random_state=23)\n",
    "\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test, y_test, test_size=0.5, random_state=23)\n",
    "\n",
    "print('X_train shape:', len(X_train))\n",
    "print('X_validate shape:', len(X_validate))\n",
    "print('X_test shape:', len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "LEARNING_RATE = 1\n",
    "TRAINING_EPOCHS = 3000\n",
    "REGULARIZATION_PARAM = tf.constant(0.001)\n",
    "# CRITICAL::: X_TRAIN[0] MUST EXIST FOR THIS TO WORK PROPERLY\n",
    "N_FEATURES = len(X_train[0])\n",
    "\n",
    "y_train = y_train.reshape(len(y_train), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, N_FEATURES], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "\n",
    "Weights = tf.Variable(tf.random_normal([N_FEATURES, 1]), name=\"Weights\")\n",
    "bias = tf.Variable(tf.random_normal([1, 1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "hypotesis = tf.sigmoid(bias + tf.matmul(X, Weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function & L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "cost = tf.reduce_mean(-tf.multiply(y, tf.log(hypotesis)) - \n",
    "                      tf.multiply(tf.subtract(1.0, y), tf.log(tf.subtract(1.0, hypotesis)))) \\\n",
    "                    + tf.multiply(REGULARIZATION_PARAM, tf.matmul(tf.transpose(Weights), Weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 6.23747206]]\n",
      "100 [[ 4.15290833]]\n",
      "200 [[ 2.87243152]]\n",
      "300 [[ 2.0606308]]\n",
      "400 [[ 1.53765106]]\n",
      "500 [[ 1.19856048]]\n",
      "600 [[ 0.97796774]]\n",
      "700 [[ 0.83410752]]\n",
      "800 [[ 0.74008286]]\n",
      "900 [[ 0.67850721]]\n",
      "1000 [[ 0.63810807]]\n",
      "1100 [[ 0.61156082]]\n",
      "1200 [[ 0.59409004]]\n",
      "1300 [[ 0.58257896]]\n",
      "1400 [[ 0.5749855]]\n",
      "1500 [[ 0.56997234]]\n",
      "1600 [[ 0.56665927]]\n",
      "1700 [[ 0.56446791]]\n",
      "1800 [[ 0.56301838]]\n",
      "1900 [[ 0.56205779]]\n",
      "2000 [[ 0.56142199]]\n",
      "2100 [[ 0.56100041]]\n",
      "2200 [[ 0.56072074]]\n",
      "2300 [[ 0.56053519]]\n",
      "2400 [[ 0.56041199]]\n",
      "2500 [[ 0.56033027]]\n",
      "2600 [[ 0.56027597]]\n",
      "2700 [[ 0.56023991]]\n",
      "2800 [[ 0.56021577]]\n",
      "2900 [[ 0.56020004]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        _, err = sess.run([train_op, cost], {X: X_train, y: y_train})\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, err)\n",
    "    W_computed, b_computed = sess.run([Weights, bias])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting and testing for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.8 %\n"
     ]
    }
   ],
   "source": [
    "#hypotesis = tf.sigmoid(bias + tf.matmul(X, Weights))\n",
    "h = tf.sigmoid(b_computed + tf.matmul(X, W_computed))\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_predict = sess.run(h, {X: X_test})\n",
    "    y_predict_predict = []\n",
    "    \n",
    "    for value in y_predict:\n",
    "        if np.mean(value) >= 0.5:\n",
    "            y_predict_predict.append(1)\n",
    "        else:\n",
    "            y_predict_predict.append(0)\n",
    "    \n",
    "    y_diff = y_predict_predict == y_test\n",
    "    \n",
    "    for value in y_diff:\n",
    "        if value:\n",
    "            cnt += 1\n",
    "    \n",
    "    print('Accuracy:', cnt/(len(y_diff)) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
